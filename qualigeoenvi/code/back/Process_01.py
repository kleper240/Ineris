# -*- coding: utf-8 -*-
"""Process_test01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iDVi4bTMGR1YbsloS-O8qpb614bQC-Kv
"""

"""# Import packages"""

from pandas.core.resample import DataError

import pandas as pd
import numpy as np
import geopandas as gpd
from datetime import datetime
import dateutil.parser
import os 
import re
from dateutil.parser import parse
from functions import convert_to_datetime, convert_to_datetime_bis, is_date
from pandas.core.resample import DataError
#os.chdir("/content/drive/My Drive")
#ls "/content/drive/My Drive/Challenge_QualiGéoEnvi/00_donnees_ineris"

"""# Connect data path"""

path_to_data = '/content/drive/My Drive/Challenge_QualiGéoEnvi/00_donnees_ineris/' 
Unique_Labels_List = os.listdir(path_to_data)
# sorted

"""# Clean data"""

# class for automatic 
class Process:
    def __init__(self, path, filename):
        self.path = path
        self.imported_data = pd.read_csv(f'{path}{filename}', sep=';', low_memory=False)
        self.processed_data = self.imported_data.copy()
        print("CSV imported")
        #self._compute_nan_P()
        self._columns_title_format()
        self.replace_space()
        self.data_desc()
# ------------- METHODS APPLIED UPON INIT ------------ #
    def _columns_title_format(self):
        data_columns = list(self.processed_data.columns)
        data_columns = [col.replace(' ', '_').capitalize() for col in data_columns]
        self.processed_data.columns = data_columns
        print("Formatted columns title")
        return self.processed_data
   
       # Function for replacing space with '_' in columns of a df
    def replace_space(self):
        df = self.processed_data.select_dtypes(include=['object'])
        df = df.stack().str.replace(' ', '_').unstack()
        # modif du 8/2/2023
        for col in df.columns:
            self.processed_data[col] = df[col]
        return self.processed_data
    # Function for quickly describing the data
    def data_desc(self):
        print(self.processed_data.dropna(how = 'all').shape) #pour voir si certaines lignes comportent des valeurs NaN partout (Non)
        return(self.processed_data.info())
        # -------------- METHODS FOR DETECTING AND DISCARDING STATISTICALLY INSIGNIFICANT COLUMNS --------- #
 ## Discard statistically insignificant features
    def _compute_nan_P(self):
        data_isna_P = (self.processed_data.isna().sum()) * 100/len(self.processed_data)
        cols = [x for x in range (self.processed_data.shape[1])]
        count =0
        print("Printing removed columns...")
        for ii in data_isna_P.index:
            count +=1
            if (data_isna_P[ii] > 55):
                cols.remove(count-1)
                print ("\t", ii)
            else:
                continue
        self.processed_data = self.processed_data.iloc[:, cols]
    # DISCARD NAN COLUMNS
    def droping_nan_variable(self, variable_col):
        # variable_Col must be given between quotations
        cond2_droping_nan_Variable = self.processed_data[variable_col].notna()
        df_new_data = self.processed_data[cond2_droping_nan_Variable]
        return df_new_data


process = Process(path=path_to_data, filename = Unique_Labels_List[0])

# eventually, filename would be the name input by user

df_clean = process.processed_data
df_clean.head()

"""utilisateur precise quel est le type de la colonne ==> stockage dans l'interface des valeurs rentrées par l'utilisateur pour chaque colonne

"""

# function to transform all "object" variable to lower case
def unify_objectvariables_case(dataset):
    object_cols = dataset.select_dtypes(include=['object'])
    for col in object_cols.columns:
        dataset[col] = dataset[col].str.lower()
    print('Case-formatted data shape is:', dataset.shape)
    return dataset
unify_objectvariables_case(df_clean)

df_clean.isna().sum()

df_clean[df_clean['Adresse_1'].isna()==True]

# Retour indices de lignes(mesures) avec valeurs manquantes sur colonnes
for col in df_clean.columns:
  if df_clean[col].isna().sum()>0:
  #exported_df = (df_clean[df_clean[col].isna()])
  #exported_df.to_csv(f'{col}')
    print(col, '\n', df_clean.loc[df_clean[col].isna(), col].index)



"""# Get column types"""
print(list(df_data_type.columns))

# Define regular expression patterns to match address-related, date-related, geo coordinates keywords
address_pattern = re.compile(r'(adresse|rue|avenue|boulevard|passage|voie|chemin|allée|ville|commune|code\s*postal|pays)', flags=re.IGNORECASE)
date_pattern = re.compile(r'(date|début|fin|année|jour|heure|période|temps|durée)', flags=re.IGNORECASE)
geo_pattern = re.compile(r'(longitude|latitude)', flags=re.IGNORECASE)

# Add a new column to indicate the type of each column
def get_column_type(col):
    if address_pattern.search(col):
        return 'adresse'
    elif date_pattern.search(col):
        return 'date'
    elif geo_pattern.search(col):
        return 'coordonnées géographiques'
    else:
        return 'autre'
# Create a dataframe to store the column names and data types
df_data_type = pd.DataFrame({'Colonne' : list(df_clean.columns)})
df_data_type['Colonne_type'] = df_data_type['Colonne'].apply(get_column_type)
# Print the dataframe
print(df_data_type)

"""## Handling Adresses in original dataframe """
cond_adresse = df_data_type[df_data_type['Colonne_type'] == 'adresse']
cond_adresse

#df_clean['adresse_complete'] = ''
for row in cond_adresse['Colonne']:
  # CONDITION DE COMPLETION DES ADRESSES
  if (df_clean[row].isna().sum() == 0):
    for ii in range(0,len(df_clean)):
      df_clean.loc[ii, 'adresse_complete'] += df_clean.loc[ii,row]
    print(row)
  else:
    print ("Pas d'adresse complète")
     
df_clean
#df_clean.loc[:, 'Adresse_1': 'Adresse_3']

"""## Handling Dates"""

# METHOD TO RETRIEVE DATE ENTRIES FROM COLUMN TITLES

# Print the column names
print(df_clean.columns)

# Define the regular expression pattern to match the keywords
print(date_pattern )

# Find the columns that match the date pattern
date_cols = list(filter(date_pattern.search, df_clean.columns))

# Print the date columns
print(date_cols)
## Detect column dates

convert_to_datetime(df_clean, date_cols)
convert_to_datetime_bis(df_clean, date_cols)
df_clean[date_cols[0]].apply(lambda x: is_date(x))
## Test that dates columns are well detected
cond_date = df_data_type[df_data_type['Colonne_type'] == 'date']
cond_date

filtered_dates = df_data_type.loc[:, cond_date['Colonne']]
for col in filtered_dates.columns:
  print(col)
  #if col.contains("fin"):
   #print('fin')
#for row in cond_date['Colonne']:
  #print(row)

""" ### Verify potential errors in dates"""
""" #### Verify that no end date is anterior to corresponding start date"""
# Define the regular expression pattern to match the keywords
date_pattern_debut = re.compile(r'début|entrée|départ|start', flags=re.IGNORECASE)
date_pattern_fin = re.compile(r'fin|sortie|arrivée|end', flags=re.IGNORECASE)
# Find the columns that match the start pattern
date_debut = list(filter(date_pattern_debut.search, df_clean.columns))
date_fin = list(filter(date_pattern_fin.search, df_clean.columns))

type(df_clean.loc[0, date_debut])


from datetime import date, datetime

today = date.today()
print("Today's date:", today, "\n Today's variable type:", type(today))

data["Date de début"].isna().sum()

datetime.strftime(df_clean.loc[0, date_debut], '%d/%m/%Y %H:%M')

#data.info()

df_clean.loc[0, 'Date de début'] - df_clean.loc[0, 'Date de fin']

dt1 = datetime.strptime(df_clean.loc[0, 'Date de début'], '%d/%m/%Y %H:%M')



dt2 = datetime.strptime(df_clean.loc[0, 'Date de fin'], '%d/%m/%Y %H:%M')

(dt2 - dt1).total_seconds()/60

df_clean[df_clean['ecart_date']>0]

np.max(df_clean['Date de début'])

type(np.max(df_clean['Date de fin']))

df_clean[df_clean['Unnamed: 15'].isnull() == False]

"""# Handling geo-Coordinates"""

# INSTALL GEOPANDAS #
# !apt install libspatialindex-dev
# !pip install rtree
# !pip install geopandas
# !pip install descartes

# !pip install descartes

import geopandas as gpd

process_3 = Process(path=path_to_data, filename = Unique_Labels_List[2])

geo_df = process_3.processed_data
geo_df

geo_gpd = gpd.read_file(path_to_data + 'Export stations Geodair.csv')
geo_gpd.head()

geo_gpd.info()

# need to convert initial file to Shapefile or geojson?? --> test

from geopandas import GeoDataFrame
from shapely.geometry import Point

geometry = [Point(xy) for xy in zip(geo_df.Longitude, geo_df.Latitude)]
df = geo_df.drop(['Longitude', 'Latitude'], axis=1)
gdf = GeoDataFrame(df, crs="EPSG:4326", geometry=geometry)

gdf





# METHOD TO SPECIFY GEOGRAPHICAL ENTRIES TYPE FROM COLUMN TITLES
GEO_cols = [col for col in clean_df.columns.str.lower() if ('latitude' in col or 'longitude' in col) ]
print(GEO_cols)

code_geo_list = []
for values in data_dict['Type']:
  if values == 'code geo':
    code_geo_list.append()

# MEthod moved to fucntions

